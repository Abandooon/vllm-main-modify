#!/usr/bin/env python3
# verify_audit.py - éªŒè¯vLLMå®¡è®¡åŠŸèƒ½çš„æµ‹è¯•è„šæœ¬

import os
import sys
import json
import time
import asyncio
from typing import Dict, Any
import requests

# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å¯ç”¨å®¡è®¡
os.environ["VLLM_STRUCTURED_OUTPUT_AUDIT"] = "true"
os.environ["VLLM_AUDIT_RECORD_ALLOWED_TOKENS"] = "false"  # é¿å…è®°å½•è¿‡å¤šæ•°æ®
os.environ["VLLM_AUDIT_RECORD_FULL_EVENTS"] = "true"
os.environ["VLLM_AUDIT_RESPONSE_LEVEL"] = "full"  # åœ¨å“åº”ä¸­åŒ…å«å®Œæ•´å®¡è®¡æ•°æ®
os.environ["VLLM_AUDIT_MAX_TRAILS"] = "1000"

print("=" * 60)
print("vLLM ç»“æ„åŒ–è¾“å‡ºå®¡è®¡åŠŸèƒ½éªŒè¯")
print("=" * 60)

# ==============================================
# æ­¥éª¤1ï¼šéªŒè¯æ¨¡å—å¯¼å…¥
# ==============================================
print("\n[æ­¥éª¤1] éªŒè¯æ¨¡å—å¯¼å…¥...")

try:
    # å¯¼å…¥å®¡è®¡æ¨¡å—
    from v1.structured_output.audit_tracker import (
        StructuredOutputAuditTracker,
        AuditEvent,
        AuditEventType,
        get_audit_tracker,
        configure_audit_tracker
    )

    print("âœ“ æˆåŠŸå¯¼å…¥ audit_tracker æ¨¡å—")

    from vllm.v1.structured_output.audit_integration import (
        StructuredOutputAuditConfig,
        initialize_audit_system
    )

    print("âœ“ æˆåŠŸå¯¼å…¥ audit_integration æ¨¡å—")

    # éªŒè¯åç«¯æ¨¡å—
    from vllm.v1.structured_output import backend_outlines
    from vllm.v1.structured_output import backend_xgrammar

    print("âœ“ æˆåŠŸå¯¼å…¥ backend æ¨¡å—")

except ImportError as e:
    print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
    print("\nè¯·ç¡®ä¿å·²æ­£ç¡®æ·»åŠ å®¡è®¡æ¨¡å—æ–‡ä»¶")
    sys.exit(1)

# ==============================================
# æ­¥éª¤2ï¼šéªŒè¯å®¡è®¡ç³»ç»Ÿåˆå§‹åŒ–
# ==============================================
print("\n[æ­¥éª¤2] éªŒè¯å®¡è®¡ç³»ç»Ÿåˆå§‹åŒ–...")

# åˆå§‹åŒ–å®¡è®¡ç³»ç»Ÿ
config = StructuredOutputAuditConfig(
    enabled=True,
    record_allowed_tokens=False,
    record_full_events=True,
    max_trails_in_memory=100,
    include_in_response=True,
    response_detail_level="full"
)

tracker = initialize_audit_system(config)
print(f"âœ“ å®¡è®¡ç³»ç»Ÿå·²åˆå§‹åŒ–")
print(f"  - å¯ç”¨çŠ¶æ€: {tracker.is_enabled()}")
print(f"  - æœ€å¤§è¿½è¸ªæ•°: {tracker.max_trails}")
print(f"  - é…ç½®è¯¦æƒ…: {json.dumps(config.to_dict(), indent=2)}")

# ==============================================
# æ­¥éª¤3ï¼šå•å…ƒæµ‹è¯•å®¡è®¡è¿½è¸ªå™¨
# ==============================================
print("\n[æ­¥éª¤3] å•å…ƒæµ‹è¯•å®¡è®¡è¿½è¸ªå™¨...")


def test_audit_tracker():
    """æµ‹è¯•å®¡è®¡è¿½è¸ªå™¨çš„åŸºæœ¬åŠŸèƒ½"""
    test_results = []

    # æµ‹è¯•1ï¼šåˆ›å»ºå®¡è®¡è¿½è¸ª
    try:
        request_id = "test_unit_001"
        tracker.start_trail(request_id, "test_backend", '{"type": "object"}')
        trail = tracker.get_trail(request_id)
        assert trail is not None, "æœªèƒ½åˆ›å»ºå®¡è®¡è¿½è¸ª"
        assert trail.request_id == request_id, "è¯·æ±‚IDä¸åŒ¹é…"
        test_results.append(("åˆ›å»ºå®¡è®¡è¿½è¸ª", True, None))
    except Exception as e:
        test_results.append(("åˆ›å»ºå®¡è®¡è¿½è¸ª", False, str(e)))

    # æµ‹è¯•2ï¼šè®°å½•tokenæ¥å—äº‹ä»¶
    try:
        tokens = [100, 200, 300]
        tracker.record_token_acceptance(
            request_id=request_id,
            tokens=tokens,
            accepted=True,
            current_state="state_1"
        )
        trail = tracker.get_trail(request_id)
        assert trail.total_tokens_generated == 3, "Tokenè®¡æ•°é”™è¯¯"
        assert len(trail.events) > 0, "æœªè®°å½•äº‹ä»¶"
        test_results.append(("è®°å½•Tokenæ¥å—", True, None))
    except Exception as e:
        test_results.append(("è®°å½•Tokenæ¥å—", False, str(e)))

    # æµ‹è¯•3ï¼šè®°å½•å›æ»šäº‹ä»¶
    try:
        tracker.record_rollback(request_id, 2, "state_2")
        trail = tracker.get_trail(request_id)
        assert trail.total_rollbacks == 1, "å›æ»šè®¡æ•°é”™è¯¯"
        test_results.append(("è®°å½•å›æ»šäº‹ä»¶", True, None))
    except Exception as e:
        test_results.append(("è®°å½•å›æ»šäº‹ä»¶", False, str(e)))

    # æµ‹è¯•4ï¼šå®Œæˆå¹¶åºåˆ—åŒ–
    try:
        tracker.finalize_trail(request_id)
        trail_dict = tracker.get_trail_dict(request_id, include_events=True)
        assert trail_dict is not None, "æ— æ³•è·å–è¿½è¸ªå­—å…¸"
        assert "events" in trail_dict, "ç¼ºå°‘äº‹ä»¶åˆ—è¡¨"
        assert trail_dict["total_tokens_generated"] == 3, "ç»Ÿè®¡æ•°æ®é”™è¯¯"
        test_results.append(("å®Œæˆå¹¶åºåˆ—åŒ–", True, None))
    except Exception as e:
        test_results.append(("å®Œæˆå¹¶åºåˆ—åŒ–", False, str(e)))

    # æ‰“å°æµ‹è¯•ç»“æœ
    print("\nå•å…ƒæµ‹è¯•ç»“æœ:")
    for test_name, passed, error in test_results:
        if passed:
            print(f"  âœ“ {test_name}")
        else:
            print(f"  âœ— {test_name}: {error}")

    return all(passed for _, passed, _ in test_results)


unit_test_passed = test_audit_tracker()
if not unit_test_passed:
    print("\nâš ï¸ å•å…ƒæµ‹è¯•æœªå®Œå…¨é€šè¿‡ï¼Œä½†ç»§ç»­è¿›è¡Œå…¶ä»–æµ‹è¯•...")

# ==============================================
# æ­¥éª¤4ï¼šæµ‹è¯•Backendé›†æˆ
# ==============================================
print("\n[æ­¥éª¤4] æµ‹è¯•Backendé›†æˆ...")


def test_backend_integration():
    """æµ‹è¯•å®¡è®¡åŠŸèƒ½ä¸å„ä¸ªbackendçš„é›†æˆ"""

    # æµ‹è¯•Outlines Backend
    try:
        from vllm.v1.structured_output.backend_outlines import OutlinesBackend
        from vllm.transformers_utils.tokenizer import get_tokenizer
        from vllm.config import VllmConfig, ModelConfig

        print("\næµ‹è¯• Outlines Backend:")

        # åˆ›å»ºä¸€ä¸ªmocké…ç½®ï¼ˆç®€åŒ–ç‰ˆï¼‰
        model_config = ModelConfig(
            model="gpt2",  # ä½¿ç”¨ç®€å•æ¨¡å‹è¿›è¡Œæµ‹è¯•
            tokenizer="gpt2",
            tokenizer_mode="auto",
            trust_remote_code=False,
            max_model_len=2048
        )

        # è·å–tokenizer
        tokenizer = get_tokenizer(
            tokenizer_name="gpt2",
            tokenizer_mode="auto"
        )

        # åˆ›å»ºVllmConfig (éœ€è¦æ ¹æ®å®é™…APIè°ƒæ•´)
        from dataclasses import dataclass
        @dataclass
        class MockVllmConfig:
            model_config: Any = model_config
            speculative_config: Any = None
            structured_outputs_config: Any = None

        vllm_config = MockVllmConfig()

        # åˆ›å»ºBackendå®ä¾‹
        backend = OutlinesBackend(
            vllm_config=vllm_config,
            tokenizer=tokenizer,
            vocab_size=tokenizer.vocab_size
        )

        print("  âœ“ Outlines Backend åˆ›å»ºæˆåŠŸ")

        # æµ‹è¯•ç¼–è¯‘è¯­æ³•
        from vllm.v1.structured_output.backend_types import StructuredOutputOptions

        json_schema = json.dumps({
            "type": "object",
            "properties": {
                "name": {"type": "string"},
                "age": {"type": "integer"}
            },
            "required": ["name", "age"]
        })

        grammar = backend.compile_grammar(
            StructuredOutputOptions.JSON,
            json_schema
        )

        print("  âœ“ Grammar ç¼–è¯‘æˆåŠŸ")

        # æµ‹è¯•å®¡è®¡ä¸Šä¸‹æ–‡è®¾ç½®
        if hasattr(grammar, 'set_audit_context'):
            grammar.set_audit_context("test_backend_001")
            print("  âœ“ å®¡è®¡ä¸Šä¸‹æ–‡è®¾ç½®æˆåŠŸ")
        else:
            print("  âš ï¸ Grammar æœªå®ç°å®¡è®¡æ¥å£")

        return True

    except ImportError as e:
        print(f"  âš ï¸ è·³è¿‡Backendæµ‹è¯• (ç¼ºå°‘ä¾èµ–): {e}")
        return False
    except Exception as e:
        print(f"  âœ— Backendæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


backend_test_passed = test_backend_integration()

# ==============================================
# æ­¥éª¤5ï¼šæµ‹è¯•vLLMæœåŠ¡å™¨é›†æˆï¼ˆå¦‚æœè¿è¡Œä¸­ï¼‰
# ==============================================
print("\n[æ­¥éª¤5] æµ‹è¯•vLLMæœåŠ¡å™¨é›†æˆ...")


def test_server_integration():
    """æµ‹è¯•ä¸è¿è¡Œä¸­çš„vLLMæœåŠ¡å™¨çš„é›†æˆ"""

    # é»˜è®¤vLLMæœåŠ¡å™¨åœ°å€
    server_url = "http://localhost:8000"

    print(f"å°è¯•è¿æ¥åˆ° {server_url}...")

    try:
        # æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ
        response = requests.get(f"{server_url}/health", timeout=2)
        if response.status_code != 200:
            print("  âš ï¸ vLLMæœåŠ¡å™¨æœªè¿è¡Œæˆ–ä¸å¯è®¿é—®")
            print("  è¦å®Œæ•´æµ‹è¯•ï¼Œè¯·å…ˆå¯åŠ¨vLLMæœåŠ¡å™¨:")
            print("  python -m vllm.entrypoints.openai.api_server \\")
            print("    --model <your-model> \\")
            print("    --port 8000")
            return False

        print("  âœ“ vLLMæœåŠ¡å™¨å¯è®¿é—®")

        # å‘é€ç»“æ„åŒ–è¾“å‡ºè¯·æ±‚
        request_data = {
            "model": "your-model",
            "prompt": "Generate a person's information:",
            "max_tokens": 100,
            "temperature": 0.7,
            "guided_json": {
                "type": "object",
                "properties": {
                    "name": {"type": "string"},
                    "age": {"type": "integer", "minimum": 0, "maximum": 120},
                    "email": {"type": "string", "format": "email"}
                },
                "required": ["name", "age", "email"]
            }
        }

        print("\n  å‘é€ç»“æ„åŒ–è¾“å‡ºè¯·æ±‚...")
        response = requests.post(
            f"{server_url}/v1/completions",
            json=request_data,
            timeout=30
        )

        if response.status_code == 200:
            result = response.json()

            # æ£€æŸ¥æ˜¯å¦åŒ…å«å®¡è®¡æ•°æ®
            if "structured_output_audit" in result:
                print("  âœ“ å“åº”åŒ…å«å®¡è®¡æ•°æ®")
                audit_data = result["structured_output_audit"]

                if "audit_trail" in audit_data:
                    trail = audit_data["audit_trail"]
                    print(f"\n  å®¡è®¡è¿½è¸ªæ‘˜è¦:")
                    print(f"    - Backend: {trail.get('backend_type', 'N/A')}")
                    print(f"    - æ€»æ­¥éª¤: {trail.get('total_steps', 0)}")
                    print(f"    - ç”ŸæˆTokenæ•°: {trail.get('total_tokens_generated', 0)}")
                    print(f"    - å›æ»šæ¬¡æ•°: {trail.get('total_rollbacks', 0)}")
                    print(f"    - é”™è¯¯æ¬¡æ•°: {trail.get('total_errors', 0)}")

                    if trail.get('events'):
                        print(f"    - äº‹ä»¶æ•°é‡: {len(trail['events'])}")
                        print(f"\n  å‰3ä¸ªäº‹ä»¶:")
                        for i, event in enumerate(trail['events'][:3], 1):
                            print(f"    {i}. {event.get('event_type', 'N/A')} at step {event.get('step_number', 0)}")
                elif "audit_summary" in audit_data:
                    summary = audit_data["audit_summary"]
                    print(f"\n  å®¡è®¡æ‘˜è¦:")
                    for key, value in summary.items():
                        print(f"    - {key}: {value}")
            else:
                print("  âš ï¸ å“åº”ä¸­æœªåŒ…å«å®¡è®¡æ•°æ®")
                print("  è¯·ç¡®è®¤æœåŠ¡å™¨æ˜¯å¦ä½¿ç”¨äº†ä¿®æ”¹åçš„ä»£ç ")

            return True
        else:
            print(f"  âœ— è¯·æ±‚å¤±è´¥: {response.status_code}")
            print(f"  å“åº”: {response.text[:200]}")
            return False

    except requests.exceptions.ConnectionError:
        print("  âš ï¸ æ— æ³•è¿æ¥åˆ°vLLMæœåŠ¡å™¨")
        return False
    except requests.exceptions.Timeout:
        print("  âš ï¸ è¯·æ±‚è¶…æ—¶")
        return False
    except Exception as e:
        print(f"  âœ— æµ‹è¯•å¤±è´¥: {e}")
        return False


server_test_passed = test_server_integration()

# ==============================================
# æ­¥éª¤6ï¼šæ€§èƒ½æµ‹è¯•
# ==============================================
print("\n[æ­¥éª¤6] æ€§èƒ½æµ‹è¯•...")


def performance_test():
    """ç®€å•çš„æ€§èƒ½æµ‹è¯•"""
    import time

    # æµ‹è¯•ç¦ç”¨å®¡è®¡çš„æ€§èƒ½
    tracker_disabled = StructuredOutputAuditTracker(enabled=False)

    start = time.perf_counter()
    for i in range(1000):
        tracker_disabled.record_token_acceptance(f"perf_{i}", [1, 2, 3], True, "state")
    disabled_time = time.perf_counter() - start

    # æµ‹è¯•å¯ç”¨å®¡è®¡çš„æ€§èƒ½
    tracker_enabled = StructuredOutputAuditTracker(enabled=True)

    start = time.perf_counter()
    for i in range(1000):
        tracker_enabled.start_trail(f"perf_{i}", "test")
        tracker_enabled.record_token_acceptance(f"perf_{i}", [1, 2, 3], True, "state")
        tracker_enabled.finalize_trail(f"perf_{i}")
    enabled_time = time.perf_counter() - start

    print(f"  ç¦ç”¨å®¡è®¡: {disabled_time * 1000:.2f}ms (1000æ¬¡æ“ä½œ)")
    print(f"  å¯ç”¨å®¡è®¡: {enabled_time * 1000:.2f}ms (1000æ¬¡æ“ä½œ)")
    print(f"  æ€§èƒ½å¼€é”€: {((enabled_time - disabled_time) / disabled_time * 100):.1f}%")

    if enabled_time < disabled_time * 2:
        print("  âœ“ æ€§èƒ½å¼€é”€åœ¨åˆç†èŒƒå›´å†… (<100%)")
        return True
    else:
        print("  âš ï¸ æ€§èƒ½å¼€é”€è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–")
        return False


perf_test_passed = performance_test()

# ==============================================
# æ€»ç»“
# ==============================================
print("\n" + "=" * 60)
print("éªŒè¯æ€»ç»“")
print("=" * 60)

test_summary = {
    "æ¨¡å—å¯¼å…¥": True,  # å¦‚æœæ‰§è¡Œåˆ°è¿™é‡Œï¼Œè¯´æ˜å¯¼å…¥æˆåŠŸ
    "å®¡è®¡ç³»ç»Ÿåˆå§‹åŒ–": tracker.is_enabled(),
    "å•å…ƒæµ‹è¯•": unit_test_passed,
    "Backendé›†æˆ": backend_test_passed,
    "æœåŠ¡å™¨é›†æˆ": server_test_passed,
    "æ€§èƒ½æµ‹è¯•": perf_test_passed
}

all_passed = all(test_summary.values())

print("\næµ‹è¯•ç»“æœ:")
for test_name, passed in test_summary.items():
    status = "âœ“" if passed else "âœ—"
    print(f"  {status} {test_name}")

if all_passed:
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å®¡è®¡åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
else:
    print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ã€‚è¯·æ£€æŸ¥:")
    if not test_summary["æœåŠ¡å™¨é›†æˆ"]:
        print("  - ç¡®ä¿vLLMæœåŠ¡å™¨æ­£åœ¨è¿è¡Œå¹¶ä½¿ç”¨ä¿®æ”¹åçš„ä»£ç ")
    if not test_summary["Backendé›†æˆ"]:
        print("  - æ£€æŸ¥Backendä¿®æ”¹æ˜¯å¦æ­£ç¡®")
    if not test_summary["å•å…ƒæµ‹è¯•"]:
        print("  - æ£€æŸ¥å®¡è®¡æ¨¡å—å®ç°")

print("\nä¸‹ä¸€æ­¥å»ºè®®:")
print("1. å¦‚æœæœåŠ¡å™¨æµ‹è¯•æœªé€šè¿‡ï¼Œå¯åŠ¨vLLMæœåŠ¡å™¨:")
print("   export VLLM_STRUCTURED_OUTPUT_AUDIT=true")
print("   python -m vllm.entrypoints.openai.api_server --model <model-name>")
print("")
print("2. ä½¿ç”¨å®é™…çš„æ¨¡å‹æµ‹è¯•:")
print("   curl http://localhost:8000/v1/completions \\")
print("     -H 'Content-Type: application/json' \\")
print("     -d '{\"model\": \"<model>\", \"prompt\": \"test\", \"guided_json\": {\"type\": \"object\"}}'")
print("")
print("3. æ£€æŸ¥å®¡è®¡æ—¥å¿—:")
print("   å¦‚æœè®¾ç½®äº† VLLM_AUDIT_PERSIST=true å’Œ VLLM_AUDIT_LOG_DIR")
print("   å¯ä»¥åœ¨æŒ‡å®šç›®å½•æŸ¥çœ‹JSONæ ¼å¼çš„å®¡è®¡æ—¥å¿—")